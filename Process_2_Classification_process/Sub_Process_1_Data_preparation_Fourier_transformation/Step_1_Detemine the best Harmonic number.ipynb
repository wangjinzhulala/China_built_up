{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ee\n",
    "import datetime\n",
    "import os\n",
    "import itertools\n",
    "import sys\n",
    "import collections\n",
    "\n",
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import geemap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ee.Initialize()\n",
    "\n",
    "# append the parent path into sys-path so wen can import necessary modules\n",
    "sys.path.append('../../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from BackGround_modules.Class_1_Make_fourier_imgs      import Make_Fourier\n",
    "from BackGround_modules.Class_2_Classify_Fourier_Img   import Classification\n",
    "from BackGround_modules.Class_3_Calculate_the_accuracy import Accuracy_assesment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step_0_Define_Basic_Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the number of random points for residule calculation\n",
    "point_num = 100\n",
    "\n",
    "# define the max harmonic number that will be tested\n",
    "harmo_num = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the time-range\n",
    "# just get one_range from every different sensors\n",
    "year_range = [(f'{i}-01-01',f'2019-12-31') for i in range(2015,2020)] + \\\n",
    "             [(f'{i}-01-01',f'2013-12-31') for i in range(2009,2014)] + \\\n",
    "             [(f'{i}-01-01',f'2010-12-31') for i in range(2006,2011)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('2015-01-01', '2019-12-31'),\n",
       " ('2016-01-01', '2019-12-31'),\n",
       " ('2017-01-01', '2019-12-31'),\n",
       " ('2018-01-01', '2019-12-31'),\n",
       " ('2019-01-01', '2019-12-31'),\n",
       " ('2009-01-01', '2013-12-31'),\n",
       " ('2010-01-01', '2013-12-31'),\n",
       " ('2011-01-01', '2013-12-31'),\n",
       " ('2012-01-01', '2013-12-31'),\n",
       " ('2013-01-01', '2013-12-31'),\n",
       " ('2006-01-01', '2010-12-31'),\n",
       " ('2007-01-01', '2010-12-31'),\n",
       " ('2008-01-01', '2010-12-31'),\n",
       " ('2009-01-01', '2010-12-31'),\n",
       " ('2010-01-01', '2010-12-31')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "year_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step_1_Triple loop to create GEE_instaces for later residule computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Construct residule instances for 华东.\n",
      "Analyzing the images of 2015_2019\n",
      "Analyzing the images of 2015_2019\n",
      "Analyzing the images of 2015_2019\n",
      "Analyzing the images of 2015_2019\n",
      "Analyzing the images of 2015_2019\n",
      "Analyzing the images of 2015_2019\n",
      "Analyzing the images of 2016_2019\n",
      "Analyzing the images of 2016_2019\n",
      "Analyzing the images of 2016_2019\n",
      "Analyzing the images of 2016_2019\n",
      "Analyzing the images of 2016_2019\n",
      "Analyzing the images of 2016_2019\n",
      "Analyzing the images of 2017_2019\n",
      "Analyzing the images of 2017_2019\n",
      "Analyzing the images of 2017_2019\n",
      "Analyzing the images of 2017_2019\n",
      "Analyzing the images of 2017_2019\n",
      "Analyzing the images of 2017_2019\n",
      "Analyzing the images of 2018_2019\n",
      "Analyzing the images of 2018_2019\n",
      "Analyzing the images of 2018_2019\n",
      "Analyzing the images of 2018_2019\n",
      "Analyzing the images of 2018_2019\n",
      "Analyzing the images of 2018_2019\n",
      "Analyzing the images of 2019_2019\n",
      "Analyzing the images of 2019_2019\n",
      "Analyzing the images of 2019_2019\n",
      "Analyzing the images of 2019_2019\n",
      "Analyzing the images of 2019_2019\n",
      "Analyzing the images of 2019_2019\n",
      "Analyzing the images of 2009_2013\n",
      "Analyzing the images of 2009_2013\n",
      "Analyzing the images of 2009_2013\n",
      "Analyzing the images of 2009_2013\n",
      "Analyzing the images of 2009_2013\n",
      "Analyzing the images of 2009_2013\n",
      "Analyzing the images of 2010_2013\n",
      "Analyzing the images of 2010_2013\n",
      "Analyzing the images of 2010_2013\n",
      "Analyzing the images of 2010_2013\n",
      "Analyzing the images of 2010_2013\n",
      "Analyzing the images of 2010_2013\n",
      "Analyzing the images of 2011_2013\n",
      "Analyzing the images of 2011_2013\n",
      "Analyzing the images of 2011_2013\n",
      "Analyzing the images of 2011_2013\n",
      "Analyzing the images of 2011_2013\n",
      "Analyzing the images of 2011_2013\n",
      "Analyzing the images of 2012_2013\n",
      "Analyzing the images of 2012_2013\n",
      "Analyzing the images of 2012_2013\n",
      "Analyzing the images of 2012_2013\n",
      "Analyzing the images of 2012_2013\n",
      "Analyzing the images of 2012_2013\n",
      "Analyzing the images of 2013_2013\n",
      "Analyzing the images of 2013_2013\n",
      "Analyzing the images of 2013_2013\n",
      "Analyzing the images of 2013_2013\n",
      "Analyzing the images of 2013_2013\n",
      "Analyzing the images of 2013_2013\n",
      "Analyzing the images of 2006_2010\n",
      "Analyzing the images of 2006_2010\n",
      "Analyzing the images of 2006_2010\n",
      "Analyzing the images of 2006_2010\n",
      "Analyzing the images of 2006_2010\n",
      "Analyzing the images of 2006_2010\n",
      "Analyzing the images of 2007_2010\n",
      "Analyzing the images of 2007_2010\n",
      "Analyzing the images of 2007_2010\n",
      "Analyzing the images of 2007_2010\n",
      "Analyzing the images of 2007_2010\n",
      "Analyzing the images of 2007_2010\n",
      "Analyzing the images of 2008_2010\n",
      "Analyzing the images of 2008_2010\n",
      "Analyzing the images of 2008_2010\n",
      "Analyzing the images of 2008_2010\n",
      "Analyzing the images of 2008_2010\n",
      "Analyzing the images of 2008_2010\n",
      "Analyzing the images of 2009_2010\n",
      "Analyzing the images of 2009_2010\n",
      "Analyzing the images of 2009_2010\n",
      "Analyzing the images of 2009_2010\n",
      "Analyzing the images of 2009_2010\n",
      "Analyzing the images of 2009_2010\n",
      "Analyzing the images of 2010_2010\n",
      "Analyzing the images of 2010_2010\n",
      "Analyzing the images of 2010_2010\n",
      "Analyzing the images of 2010_2010\n",
      "Analyzing the images of 2010_2010\n",
      "Analyzing the images of 2010_2010\n",
      "Construct residule instances for 东北.\n",
      "Analyzing the images of 2015_2019\n",
      "Analyzing the images of 2015_2019\n",
      "Analyzing the images of 2015_2019\n",
      "Analyzing the images of 2015_2019\n",
      "Analyzing the images of 2015_2019\n",
      "Analyzing the images of 2015_2019\n",
      "Analyzing the images of 2016_2019\n",
      "Analyzing the images of 2016_2019\n",
      "Analyzing the images of 2016_2019\n",
      "Analyzing the images of 2016_2019\n",
      "Analyzing the images of 2016_2019\n",
      "Analyzing the images of 2016_2019\n",
      "Analyzing the images of 2017_2019\n",
      "Analyzing the images of 2017_2019\n",
      "Analyzing the images of 2017_2019\n",
      "Analyzing the images of 2017_2019\n",
      "Analyzing the images of 2017_2019\n",
      "Analyzing the images of 2017_2019\n",
      "Analyzing the images of 2018_2019\n",
      "Analyzing the images of 2018_2019\n",
      "Analyzing the images of 2018_2019\n",
      "Analyzing the images of 2018_2019\n",
      "Analyzing the images of 2018_2019\n",
      "Analyzing the images of 2018_2019\n",
      "Analyzing the images of 2019_2019\n",
      "Analyzing the images of 2019_2019\n",
      "Analyzing the images of 2019_2019\n",
      "Analyzing the images of 2019_2019\n",
      "Analyzing the images of 2019_2019\n",
      "Analyzing the images of 2019_2019\n",
      "Analyzing the images of 2009_2013\n",
      "Analyzing the images of 2009_2013\n",
      "Analyzing the images of 2009_2013\n",
      "Analyzing the images of 2009_2013\n",
      "Analyzing the images of 2009_2013\n",
      "Analyzing the images of 2009_2013\n",
      "Analyzing the images of 2010_2013\n",
      "Analyzing the images of 2010_2013\n",
      "Analyzing the images of 2010_2013\n",
      "Analyzing the images of 2010_2013\n",
      "Analyzing the images of 2010_2013\n",
      "Analyzing the images of 2010_2013\n",
      "Analyzing the images of 2011_2013\n",
      "Analyzing the images of 2011_2013\n",
      "Analyzing the images of 2011_2013\n",
      "Analyzing the images of 2011_2013\n",
      "Analyzing the images of 2011_2013\n",
      "Analyzing the images of 2011_2013\n",
      "Analyzing the images of 2012_2013\n",
      "Analyzing the images of 2012_2013\n",
      "Analyzing the images of 2012_2013\n",
      "Analyzing the images of 2012_2013\n",
      "Analyzing the images of 2012_2013\n",
      "Analyzing the images of 2012_2013\n",
      "Analyzing the images of 2013_2013\n",
      "Analyzing the images of 2013_2013\n",
      "Analyzing the images of 2013_2013\n",
      "Analyzing the images of 2013_2013\n",
      "Analyzing the images of 2013_2013\n",
      "Analyzing the images of 2013_2013\n",
      "Analyzing the images of 2006_2010\n",
      "Analyzing the images of 2006_2010\n",
      "Analyzing the images of 2006_2010\n",
      "Analyzing the images of 2006_2010\n",
      "Analyzing the images of 2006_2010\n",
      "Analyzing the images of 2006_2010\n",
      "Analyzing the images of 2007_2010\n",
      "Analyzing the images of 2007_2010\n",
      "Analyzing the images of 2007_2010\n",
      "Analyzing the images of 2007_2010\n",
      "Analyzing the images of 2007_2010\n",
      "Analyzing the images of 2007_2010\n",
      "Analyzing the images of 2008_2010\n",
      "Analyzing the images of 2008_2010\n",
      "Analyzing the images of 2008_2010\n",
      "Analyzing the images of 2008_2010\n",
      "Analyzing the images of 2008_2010\n",
      "Analyzing the images of 2008_2010\n",
      "Analyzing the images of 2009_2010\n",
      "Analyzing the images of 2009_2010\n",
      "Analyzing the images of 2009_2010\n",
      "Analyzing the images of 2009_2010\n",
      "Analyzing the images of 2009_2010\n",
      "Analyzing the images of 2009_2010\n",
      "Analyzing the images of 2010_2010\n",
      "Analyzing the images of 2010_2010\n",
      "Analyzing the images of 2010_2010\n",
      "Analyzing the images of 2010_2010\n",
      "Analyzing the images of 2010_2010\n",
      "Analyzing the images of 2010_2010\n",
      "Construct residule instances for 中南.\n",
      "Analyzing the images of 2015_2019\n",
      "Analyzing the images of 2015_2019\n",
      "Analyzing the images of 2015_2019\n",
      "Analyzing the images of 2015_2019\n",
      "Analyzing the images of 2015_2019\n",
      "Analyzing the images of 2015_2019\n",
      "Analyzing the images of 2016_2019\n",
      "Analyzing the images of 2016_2019\n",
      "Analyzing the images of 2016_2019\n",
      "Analyzing the images of 2016_2019\n",
      "Analyzing the images of 2016_2019\n",
      "Analyzing the images of 2016_2019\n",
      "Analyzing the images of 2017_2019\n",
      "Analyzing the images of 2017_2019\n",
      "Analyzing the images of 2017_2019\n",
      "Analyzing the images of 2017_2019\n",
      "Analyzing the images of 2017_2019\n",
      "Analyzing the images of 2017_2019\n",
      "Analyzing the images of 2018_2019\n",
      "Analyzing the images of 2018_2019\n",
      "Analyzing the images of 2018_2019\n",
      "Analyzing the images of 2018_2019\n",
      "Analyzing the images of 2018_2019\n",
      "Analyzing the images of 2018_2019\n",
      "Analyzing the images of 2019_2019\n",
      "Analyzing the images of 2019_2019\n",
      "Analyzing the images of 2019_2019\n",
      "Analyzing the images of 2019_2019\n",
      "Analyzing the images of 2019_2019\n",
      "Analyzing the images of 2019_2019\n",
      "Analyzing the images of 2009_2013\n",
      "Analyzing the images of 2009_2013\n",
      "Analyzing the images of 2009_2013\n",
      "Analyzing the images of 2009_2013\n",
      "Analyzing the images of 2009_2013\n",
      "Analyzing the images of 2009_2013\n",
      "Analyzing the images of 2010_2013\n",
      "Analyzing the images of 2010_2013\n",
      "Analyzing the images of 2010_2013\n",
      "Analyzing the images of 2010_2013\n",
      "Analyzing the images of 2010_2013\n",
      "Analyzing the images of 2010_2013\n",
      "Analyzing the images of 2011_2013\n",
      "Analyzing the images of 2011_2013\n",
      "Analyzing the images of 2011_2013\n",
      "Analyzing the images of 2011_2013\n",
      "Analyzing the images of 2011_2013\n",
      "Analyzing the images of 2011_2013\n",
      "Analyzing the images of 2012_2013\n",
      "Analyzing the images of 2012_2013\n",
      "Analyzing the images of 2012_2013\n",
      "Analyzing the images of 2012_2013\n",
      "Analyzing the images of 2012_2013\n",
      "Analyzing the images of 2012_2013\n",
      "Analyzing the images of 2013_2013\n",
      "Analyzing the images of 2013_2013\n",
      "Analyzing the images of 2013_2013\n",
      "Analyzing the images of 2013_2013\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing the images of 2013_2013\n",
      "Analyzing the images of 2013_2013\n",
      "Analyzing the images of 2006_2010\n",
      "Analyzing the images of 2006_2010\n",
      "Analyzing the images of 2006_2010\n",
      "Analyzing the images of 2006_2010\n",
      "Analyzing the images of 2006_2010\n",
      "Analyzing the images of 2006_2010\n",
      "Analyzing the images of 2007_2010\n",
      "Analyzing the images of 2007_2010\n",
      "Analyzing the images of 2007_2010\n",
      "Analyzing the images of 2007_2010\n",
      "Analyzing the images of 2007_2010\n",
      "Analyzing the images of 2007_2010\n",
      "Analyzing the images of 2008_2010\n",
      "Analyzing the images of 2008_2010\n",
      "Analyzing the images of 2008_2010\n",
      "Analyzing the images of 2008_2010\n",
      "Analyzing the images of 2008_2010\n",
      "Analyzing the images of 2008_2010\n",
      "Analyzing the images of 2009_2010\n",
      "Analyzing the images of 2009_2010\n",
      "Analyzing the images of 2009_2010\n",
      "Analyzing the images of 2009_2010\n",
      "Analyzing the images of 2009_2010\n",
      "Analyzing the images of 2009_2010\n",
      "Analyzing the images of 2010_2010\n",
      "Analyzing the images of 2010_2010\n",
      "Analyzing the images of 2010_2010\n",
      "Analyzing the images of 2010_2010\n",
      "Analyzing the images of 2010_2010\n",
      "Analyzing the images of 2010_2010\n",
      "Construct residule instances for 华北.\n",
      "Analyzing the images of 2015_2019\n",
      "Analyzing the images of 2015_2019\n",
      "Analyzing the images of 2015_2019\n",
      "Analyzing the images of 2015_2019\n",
      "Analyzing the images of 2015_2019\n",
      "Analyzing the images of 2015_2019\n",
      "Analyzing the images of 2016_2019\n",
      "Analyzing the images of 2016_2019\n",
      "Analyzing the images of 2016_2019\n",
      "Analyzing the images of 2016_2019\n",
      "Analyzing the images of 2016_2019\n",
      "Analyzing the images of 2016_2019\n",
      "Analyzing the images of 2017_2019\n",
      "Analyzing the images of 2017_2019\n",
      "Analyzing the images of 2017_2019\n",
      "Analyzing the images of 2017_2019\n",
      "Analyzing the images of 2017_2019\n",
      "Analyzing the images of 2017_2019\n",
      "Analyzing the images of 2018_2019\n",
      "Analyzing the images of 2018_2019\n",
      "Analyzing the images of 2018_2019\n",
      "Analyzing the images of 2018_2019\n",
      "Analyzing the images of 2018_2019\n",
      "Analyzing the images of 2018_2019\n",
      "Analyzing the images of 2019_2019\n",
      "Analyzing the images of 2019_2019\n",
      "Analyzing the images of 2019_2019\n",
      "Analyzing the images of 2019_2019\n",
      "Analyzing the images of 2019_2019\n",
      "Analyzing the images of 2019_2019\n",
      "Analyzing the images of 2009_2013\n",
      "Analyzing the images of 2009_2013\n",
      "Analyzing the images of 2009_2013\n",
      "Analyzing the images of 2009_2013\n",
      "Analyzing the images of 2009_2013\n",
      "Analyzing the images of 2009_2013\n",
      "Analyzing the images of 2010_2013\n",
      "Analyzing the images of 2010_2013\n",
      "Analyzing the images of 2010_2013\n",
      "Analyzing the images of 2010_2013\n",
      "Analyzing the images of 2010_2013\n",
      "Analyzing the images of 2010_2013\n",
      "Analyzing the images of 2011_2013\n",
      "Analyzing the images of 2011_2013\n",
      "Analyzing the images of 2011_2013\n",
      "Analyzing the images of 2011_2013\n",
      "Analyzing the images of 2011_2013\n",
      "Analyzing the images of 2011_2013\n",
      "Analyzing the images of 2012_2013\n",
      "Analyzing the images of 2012_2013\n",
      "Analyzing the images of 2012_2013\n",
      "Analyzing the images of 2012_2013\n",
      "Analyzing the images of 2012_2013\n",
      "Analyzing the images of 2012_2013\n",
      "Analyzing the images of 2013_2013\n",
      "Analyzing the images of 2013_2013\n",
      "Analyzing the images of 2013_2013\n",
      "Analyzing the images of 2013_2013\n",
      "Analyzing the images of 2013_2013\n",
      "Analyzing the images of 2013_2013\n",
      "Analyzing the images of 2006_2010\n",
      "Analyzing the images of 2006_2010\n",
      "Analyzing the images of 2006_2010\n",
      "Analyzing the images of 2006_2010\n",
      "Analyzing the images of 2006_2010\n",
      "Analyzing the images of 2006_2010\n",
      "Analyzing the images of 2007_2010\n",
      "Analyzing the images of 2007_2010\n",
      "Analyzing the images of 2007_2010\n",
      "Analyzing the images of 2007_2010\n",
      "Analyzing the images of 2007_2010\n",
      "Analyzing the images of 2007_2010\n",
      "Analyzing the images of 2008_2010\n",
      "Analyzing the images of 2008_2010\n",
      "Analyzing the images of 2008_2010\n",
      "Analyzing the images of 2008_2010\n",
      "Analyzing the images of 2008_2010\n",
      "Analyzing the images of 2008_2010\n",
      "Analyzing the images of 2009_2010\n",
      "Analyzing the images of 2009_2010\n",
      "Analyzing the images of 2009_2010\n",
      "Analyzing the images of 2009_2010\n",
      "Analyzing the images of 2009_2010\n",
      "Analyzing the images of 2009_2010\n",
      "Analyzing the images of 2010_2010\n",
      "Analyzing the images of 2010_2010\n",
      "Analyzing the images of 2010_2010\n",
      "Analyzing the images of 2010_2010\n",
      "Analyzing the images of 2010_2010\n",
      "Analyzing the images of 2010_2010\n",
      "Construct residule instances for 西北.\n",
      "Analyzing the images of 2015_2019\n",
      "Analyzing the images of 2015_2019\n",
      "Analyzing the images of 2015_2019\n",
      "Analyzing the images of 2015_2019\n",
      "Analyzing the images of 2015_2019\n",
      "Analyzing the images of 2015_2019\n",
      "Analyzing the images of 2016_2019\n",
      "Analyzing the images of 2016_2019\n",
      "Analyzing the images of 2016_2019\n",
      "Analyzing the images of 2016_2019\n",
      "Analyzing the images of 2016_2019\n",
      "Analyzing the images of 2016_2019\n",
      "Analyzing the images of 2017_2019\n",
      "Analyzing the images of 2017_2019\n",
      "Analyzing the images of 2017_2019\n",
      "Analyzing the images of 2017_2019\n",
      "Analyzing the images of 2017_2019\n",
      "Analyzing the images of 2017_2019\n",
      "Analyzing the images of 2018_2019\n",
      "Analyzing the images of 2018_2019\n",
      "Analyzing the images of 2018_2019\n",
      "Analyzing the images of 2018_2019\n",
      "Analyzing the images of 2018_2019\n",
      "Analyzing the images of 2018_2019\n",
      "Analyzing the images of 2019_2019\n",
      "Analyzing the images of 2019_2019\n",
      "Analyzing the images of 2019_2019\n",
      "Analyzing the images of 2019_2019\n",
      "Analyzing the images of 2019_2019\n",
      "Analyzing the images of 2019_2019\n",
      "Analyzing the images of 2009_2013\n",
      "Analyzing the images of 2009_2013\n",
      "Analyzing the images of 2009_2013\n",
      "Analyzing the images of 2009_2013\n",
      "Analyzing the images of 2009_2013\n",
      "Analyzing the images of 2009_2013\n",
      "Analyzing the images of 2010_2013\n",
      "Analyzing the images of 2010_2013\n",
      "Analyzing the images of 2010_2013\n",
      "Analyzing the images of 2010_2013\n",
      "Analyzing the images of 2010_2013\n",
      "Analyzing the images of 2010_2013\n",
      "Analyzing the images of 2011_2013\n",
      "Analyzing the images of 2011_2013\n",
      "Analyzing the images of 2011_2013\n",
      "Analyzing the images of 2011_2013\n",
      "Analyzing the images of 2011_2013\n",
      "Analyzing the images of 2011_2013\n",
      "Analyzing the images of 2012_2013\n",
      "Analyzing the images of 2012_2013\n",
      "Analyzing the images of 2012_2013\n",
      "Analyzing the images of 2012_2013\n",
      "Analyzing the images of 2012_2013\n",
      "Analyzing the images of 2012_2013\n",
      "Analyzing the images of 2013_2013\n",
      "Analyzing the images of 2013_2013\n",
      "Analyzing the images of 2013_2013\n",
      "Analyzing the images of 2013_2013\n",
      "Analyzing the images of 2013_2013\n",
      "Analyzing the images of 2013_2013\n",
      "Analyzing the images of 2006_2010\n",
      "Analyzing the images of 2006_2010\n",
      "Analyzing the images of 2006_2010\n",
      "Analyzing the images of 2006_2010\n",
      "Analyzing the images of 2006_2010\n",
      "Analyzing the images of 2006_2010\n",
      "Analyzing the images of 2007_2010\n",
      "Analyzing the images of 2007_2010\n",
      "Analyzing the images of 2007_2010\n",
      "Analyzing the images of 2007_2010\n",
      "Analyzing the images of 2007_2010\n",
      "Analyzing the images of 2007_2010\n",
      "Analyzing the images of 2008_2010\n",
      "Analyzing the images of 2008_2010\n",
      "Analyzing the images of 2008_2010\n",
      "Analyzing the images of 2008_2010\n",
      "Analyzing the images of 2008_2010\n",
      "Analyzing the images of 2008_2010\n",
      "Analyzing the images of 2009_2010\n",
      "Analyzing the images of 2009_2010\n",
      "Analyzing the images of 2009_2010\n",
      "Analyzing the images of 2009_2010\n",
      "Analyzing the images of 2009_2010\n",
      "Analyzing the images of 2009_2010\n",
      "Analyzing the images of 2010_2010\n",
      "Analyzing the images of 2010_2010\n",
      "Analyzing the images of 2010_2010\n",
      "Analyzing the images of 2010_2010\n",
      "Analyzing the images of 2010_2010\n",
      "Analyzing the images of 2010_2010\n",
      "Construct residule instances for 西南.\n",
      "Analyzing the images of 2015_2019\n",
      "Analyzing the images of 2015_2019\n",
      "Analyzing the images of 2015_2019\n",
      "Analyzing the images of 2015_2019\n",
      "Analyzing the images of 2015_2019\n",
      "Analyzing the images of 2015_2019\n",
      "Analyzing the images of 2016_2019\n",
      "Analyzing the images of 2016_2019\n",
      "Analyzing the images of 2016_2019\n",
      "Analyzing the images of 2016_2019\n",
      "Analyzing the images of 2016_2019\n",
      "Analyzing the images of 2016_2019\n",
      "Analyzing the images of 2017_2019\n",
      "Analyzing the images of 2017_2019\n",
      "Analyzing the images of 2017_2019\n",
      "Analyzing the images of 2017_2019\n",
      "Analyzing the images of 2017_2019\n",
      "Analyzing the images of 2017_2019\n",
      "Analyzing the images of 2018_2019\n",
      "Analyzing the images of 2018_2019\n",
      "Analyzing the images of 2018_2019\n",
      "Analyzing the images of 2018_2019\n",
      "Analyzing the images of 2018_2019\n",
      "Analyzing the images of 2018_2019\n",
      "Analyzing the images of 2019_2019\n",
      "Analyzing the images of 2019_2019\n",
      "Analyzing the images of 2019_2019\n",
      "Analyzing the images of 2019_2019\n",
      "Analyzing the images of 2019_2019\n",
      "Analyzing the images of 2019_2019\n",
      "Analyzing the images of 2009_2013\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing the images of 2009_2013\n",
      "Analyzing the images of 2009_2013\n",
      "Analyzing the images of 2009_2013\n",
      "Analyzing the images of 2009_2013\n",
      "Analyzing the images of 2009_2013\n",
      "Analyzing the images of 2010_2013\n",
      "Analyzing the images of 2010_2013\n",
      "Analyzing the images of 2010_2013\n",
      "Analyzing the images of 2010_2013\n",
      "Analyzing the images of 2010_2013\n",
      "Analyzing the images of 2010_2013\n",
      "Analyzing the images of 2011_2013\n",
      "Analyzing the images of 2011_2013\n",
      "Analyzing the images of 2011_2013\n",
      "Analyzing the images of 2011_2013\n",
      "Analyzing the images of 2011_2013\n",
      "Analyzing the images of 2011_2013\n",
      "Analyzing the images of 2012_2013\n",
      "Analyzing the images of 2012_2013\n",
      "Analyzing the images of 2012_2013\n",
      "Analyzing the images of 2012_2013\n",
      "Analyzing the images of 2012_2013\n",
      "Analyzing the images of 2012_2013\n",
      "Analyzing the images of 2013_2013\n",
      "Analyzing the images of 2013_2013\n",
      "Analyzing the images of 2013_2013\n",
      "Analyzing the images of 2013_2013\n",
      "Analyzing the images of 2013_2013\n",
      "Analyzing the images of 2013_2013\n",
      "Analyzing the images of 2006_2010\n",
      "Analyzing the images of 2006_2010\n",
      "Analyzing the images of 2006_2010\n",
      "Analyzing the images of 2006_2010\n",
      "Analyzing the images of 2006_2010\n",
      "Analyzing the images of 2006_2010\n",
      "Analyzing the images of 2007_2010\n",
      "Analyzing the images of 2007_2010\n",
      "Analyzing the images of 2007_2010\n",
      "Analyzing the images of 2007_2010\n",
      "Analyzing the images of 2007_2010\n",
      "Analyzing the images of 2007_2010\n",
      "Analyzing the images of 2008_2010\n",
      "Analyzing the images of 2008_2010\n",
      "Analyzing the images of 2008_2010\n",
      "Analyzing the images of 2008_2010\n",
      "Analyzing the images of 2008_2010\n",
      "Analyzing the images of 2008_2010\n",
      "Analyzing the images of 2009_2010\n",
      "Analyzing the images of 2009_2010\n",
      "Analyzing the images of 2009_2010\n",
      "Analyzing the images of 2009_2010\n",
      "Analyzing the images of 2009_2010\n",
      "Analyzing the images of 2009_2010\n",
      "Analyzing the images of 2010_2010\n",
      "Analyzing the images of 2010_2010\n",
      "Analyzing the images of 2010_2010\n",
      "Analyzing the images of 2010_2010\n",
      "Analyzing the images of 2010_2010\n",
      "Analyzing the images of 2010_2010\n"
     ]
    }
   ],
   "source": [
    "# get the regions\n",
    "regions = ['华东','东北','中南','华北','西北','西南']\n",
    "\n",
    "# innitialize the list that holds residule vlaue\n",
    "point_with_residule = {}\n",
    "\n",
    "\n",
    "for region in regions:\n",
    "\n",
    "    # imprt research_area boundary\n",
    "    Target_area = ee.FeatureCollection(\"users/wangjinzhulala/China_built_up/01_Boundary_shp/China_zone\")\\\n",
    "                          .filterMetadata('NAME1','equals',region)\n",
    "    \n",
    "    # get random points\n",
    "    random_point = ee.FeatureCollection.randomPoints(Target_area, point_num, 101).toList(point_num).getInfo()\n",
    "    \n",
    "    # print out the region under processing\n",
    "    print(f'Construct residule instances for {region}.')\n",
    "\n",
    "\n",
    "    # __________________________________loop through the year_range________________________________\n",
    "    for span in year_range:\n",
    "\n",
    "        # define the start and end of time\n",
    "        start = span[0]\n",
    "        end   = span[1]\n",
    "        spane_len = str(int(end[:4]) - int(start[:4]))\n",
    "\n",
    "        # get span_name for export\n",
    "        split = '-'\n",
    "        span_name = f'{start.split(split)[0]}_{end.split(split)[0]}'\n",
    "\n",
    "\n",
    "        # __________________________________loop through harmonics ________________________________\n",
    "        for harmonic in range(1,harmo_num + 1):\n",
    "\n",
    "            # make a residule img \n",
    "            Fourier_making = Make_Fourier(start_date=start,end_date=end,harmonics=harmonic,area=Target_area)\n",
    "            Fourier_making.Stp_1_Create_hamonic_names()\n",
    "            Fourier_making.Stp_2_Add_harmonics()\n",
    "            Fourier_making.Stp_3_Harmonic_fit()\n",
    "\n",
    "            # get the Residule img.\n",
    "            Residule_img = ee.ImageCollection(Fourier_making.harmonicTrendResidule)\n",
    "\n",
    "\n",
    "            # ______________________________loop through each point _______________________________\n",
    "            for pt in random_point:\n",
    "                \n",
    "                # get the id of the point\n",
    "                pt_id = pt['id']\n",
    "\n",
    "                # calculate the residule, this is ght 3rd level of the dict\n",
    "                point_with_residule[(region,span_name,harmonic,int(pt_id))] = Residule_img.getRegion(pt['geometry'],30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step_2_Unpack the residule values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unpack ('华东', '2015_2019', 1, 0) successful!  ==>1/54000\n",
      "Unpack ('华东', '2015_2019', 1, 1) successful!  ==>2/54000\n",
      "Unpack ('华东', '2015_2019', 1, 2) successful!  ==>3/54000\n",
      "Unpack ('华东', '2015_2019', 1, 3) successful!  ==>4/54000\n",
      "Unpack ('华东', '2015_2019', 1, 4) successful!  ==>5/54000\n",
      "Unpack ('华东', '2015_2019', 1, 5) successful!  ==>6/54000\n",
      "Unpack ('华东', '2015_2019', 1, 6) successful!  ==>7/54000\n",
      "Unpack ('华东', '2015_2019', 1, 7) successful!  ==>8/54000\n",
      "Unpack ('华东', '2015_2019', 1, 8) successful!  ==>9/54000\n",
      "Unpack ('华东', '2015_2019', 1, 9) successful!  ==>10/54000\n",
      "Unpack ('华东', '2015_2019', 1, 10) successful!  ==>11/54000\n",
      "Unpack ('华东', '2015_2019', 1, 11) successful!  ==>12/54000\n",
      "Unpack ('华东', '2015_2019', 1, 12) successful!  ==>13/54000\n",
      "Unpack ('华东', '2015_2019', 1, 13) successful!  ==>14/54000\n",
      "Unpack ('华东', '2015_2019', 1, 14) successful!  ==>15/54000\n",
      "Unpack ('华东', '2015_2019', 1, 15) successful!  ==>16/54000\n",
      "Unpack ('华东', '2015_2019', 1, 16) successful!  ==>17/54000\n",
      "Unpack ('华东', '2015_2019', 1, 17) successful!  ==>18/54000\n",
      "Unpack ('华东', '2015_2019', 1, 18) successful!  ==>19/54000\n",
      "Unpack ('华东', '2015_2019', 1, 19) successful!  ==>20/54000\n",
      "Unpack ('华东', '2015_2019', 1, 20) successful!  ==>21/54000\n",
      "Unpack ('华东', '2015_2019', 1, 21) successful!  ==>22/54000\n",
      "Unpack ('华东', '2015_2019', 1, 22) successful!  ==>23/54000\n",
      "Unpack ('华东', '2015_2019', 1, 23) successful!  ==>24/54000\n",
      "Unpack ('华东', '2015_2019', 1, 24) successful!  ==>25/54000\n",
      "Unpack ('华东', '2015_2019', 1, 25) successful!  ==>26/54000\n",
      "Unpack ('华东', '2015_2019', 1, 26) successful!  ==>27/54000\n",
      "Unpack ('华东', '2015_2019', 1, 27) successful!  ==>28/54000\n",
      "Unpack ('华东', '2015_2019', 1, 28) successful!  ==>29/54000\n",
      "Unpack ('华东', '2015_2019', 1, 29) successful!  ==>30/54000\n",
      "Unpack ('华东', '2015_2019', 1, 30) successful!  ==>31/54000\n",
      "Unpack ('华东', '2015_2019', 1, 31) successful!  ==>32/54000\n",
      "Unpack ('华东', '2015_2019', 1, 32) successful!  ==>33/54000\n",
      "Unpack ('华东', '2015_2019', 1, 33) successful!  ==>34/54000\n",
      "Unpack ('华东', '2015_2019', 1, 34) successful!  ==>35/54000\n",
      "Unpack ('华东', '2015_2019', 1, 35) successful!  ==>36/54000\n",
      "Unpack ('华东', '2015_2019', 1, 36) successful!  ==>37/54000\n",
      "Unpack ('华东', '2015_2019', 1, 37) successful!  ==>38/54000\n",
      "Unpack ('华东', '2015_2019', 1, 38) successful!  ==>39/54000\n",
      "Unpack ('华东', '2015_2019', 1, 39) successful!  ==>40/54000\n",
      "Unpack ('华东', '2015_2019', 1, 40) successful!  ==>41/54000\n",
      "Unpack ('华东', '2015_2019', 1, 41) successful!  ==>42/54000\n",
      "Unpack ('华东', '2015_2019', 1, 42) successful!  ==>43/54000\n",
      "Unpack ('华东', '2015_2019', 1, 43) successful!  ==>44/54000\n",
      "Unpack ('华东', '2015_2019', 1, 44) successful!  ==>45/54000\n",
      "Unpack ('华东', '2015_2019', 1, 45) successful!  ==>46/54000\n",
      "Unpack ('华东', '2015_2019', 1, 46) successful!  ==>47/54000\n",
      "Unpack ('华东', '2015_2019', 1, 47) successful!  ==>48/54000\n",
      "Unpack ('华东', '2015_2019', 1, 48) successful!  ==>49/54000\n",
      "Unpack ('华东', '2015_2019', 1, 49) successful!  ==>50/54000\n",
      "Unpack ('华东', '2015_2019', 1, 50) successful!  ==>51/54000\n",
      "Unpack ('华东', '2015_2019', 1, 51) successful!  ==>52/54000\n",
      "Unpack ('华东', '2015_2019', 1, 52) successful!  ==>53/54000\n",
      "Unpack ('华东', '2015_2019', 1, 53) successful!  ==>54/54000\n",
      "Unpack ('华东', '2015_2019', 1, 54) successful!  ==>55/54000\n",
      "Unpack ('华东', '2015_2019', 1, 55) successful!  ==>56/54000\n",
      "Unpack ('华东', '2015_2019', 1, 56) successful!  ==>57/54000\n",
      "Unpack ('华东', '2015_2019', 1, 57) successful!  ==>58/54000\n",
      "Unpack ('华东', '2015_2019', 1, 58) successful!  ==>59/54000\n",
      "Unpack ('华东', '2015_2019', 1, 59) successful!  ==>60/54000\n",
      "Unpack ('华东', '2015_2019', 1, 60) successful!  ==>61/54000\n",
      "Unpack ('华东', '2015_2019', 1, 61) successful!  ==>62/54000\n",
      "Unpack ('华东', '2015_2019', 1, 62) successful!  ==>63/54000\n",
      "Unpack ('华东', '2015_2019', 1, 63) successful!  ==>64/54000\n",
      "Unpack ('华东', '2015_2019', 1, 64) successful!  ==>65/54000\n",
      "Unpack ('华东', '2015_2019', 1, 65) successful!  ==>66/54000\n",
      "Unpack ('华东', '2015_2019', 1, 66) successful!  ==>67/54000\n",
      "Unpack ('华东', '2015_2019', 1, 67) successful!  ==>68/54000\n",
      "Unpack ('华东', '2015_2019', 1, 68) successful!  ==>69/54000\n",
      "Unpack ('华东', '2015_2019', 1, 69) successful!  ==>70/54000\n",
      "Unpack ('华东', '2015_2019', 1, 70) successful!  ==>71/54000\n",
      "Unpack ('华东', '2015_2019', 1, 71) successful!  ==>72/54000\n",
      "Unpack ('华东', '2015_2019', 1, 72) successful!  ==>73/54000\n",
      "Unpack ('华东', '2015_2019', 1, 73) successful!  ==>74/54000\n",
      "Unpack ('华东', '2015_2019', 1, 74) successful!  ==>75/54000\n",
      "Unpack ('华东', '2015_2019', 1, 75) successful!  ==>76/54000\n",
      "Unpack ('华东', '2015_2019', 1, 76) successful!  ==>77/54000\n",
      "Unpack ('华东', '2015_2019', 1, 77) successful!  ==>78/54000\n",
      "Unpack ('华东', '2015_2019', 1, 78) successful!  ==>79/54000\n",
      "Unpack ('华东', '2015_2019', 1, 79) successful!  ==>80/54000\n",
      "Unpack ('华东', '2015_2019', 1, 80) successful!  ==>81/54000\n",
      "Unpack ('华东', '2015_2019', 1, 81) successful!  ==>82/54000\n",
      "Unpack ('华东', '2015_2019', 1, 82) successful!  ==>83/54000\n",
      "Unpack ('华东', '2015_2019', 1, 83) successful!  ==>84/54000\n",
      "Unpack ('华东', '2015_2019', 1, 84) successful!  ==>85/54000\n",
      "Unpack ('华东', '2015_2019', 1, 85) successful!  ==>86/54000\n",
      "Unpack ('华东', '2015_2019', 1, 86) successful!  ==>87/54000\n",
      "Unpack ('华东', '2015_2019', 1, 87) successful!  ==>88/54000\n",
      "Unpack ('华东', '2015_2019', 1, 88) successful!  ==>89/54000\n",
      "Unpack ('华东', '2015_2019', 1, 89) successful!  ==>90/54000\n",
      "Unpack ('华东', '2015_2019', 1, 90) successful!  ==>91/54000\n",
      "Unpack ('华东', '2015_2019', 1, 91) successful!  ==>92/54000\n",
      "Unpack ('华东', '2015_2019', 1, 92) successful!  ==>93/54000\n",
      "Unpack ('华东', '2015_2019', 1, 93) successful!  ==>94/54000\n",
      "Unpack ('华东', '2015_2019', 1, 94) successful!  ==>95/54000\n",
      "Unpack ('华东', '2015_2019', 1, 95) successful!  ==>96/54000\n",
      "Unpack ('华东', '2015_2019', 1, 96) successful!  ==>97/54000\n",
      "Unpack ('华东', '2015_2019', 1, 97) successful!  ==>98/54000\n",
      "Unpack ('华东', '2015_2019', 1, 98) successful!  ==>99/54000\n",
      "Unpack ('华东', '2015_2019', 1, 99) successful!  ==>100/54000\n",
      "Unpack ('华东', '2015_2019', 2, 0) successful!  ==>101/54000\n",
      "Unpack ('华东', '2015_2019', 2, 1) successful!  ==>102/54000\n",
      "Unpack ('华东', '2015_2019', 2, 2) successful!  ==>103/54000\n",
      "Unpack ('华东', '2015_2019', 2, 3) successful!  ==>104/54000\n",
      "Unpack ('华东', '2015_2019', 2, 4) successful!  ==>105/54000\n",
      "Unpack ('华东', '2015_2019', 2, 5) successful!  ==>106/54000\n",
      "Unpack ('华东', '2015_2019', 2, 6) successful!  ==>107/54000\n",
      "Unpack ('华东', '2015_2019', 2, 7) successful!  ==>108/54000\n",
      "Unpack ('华东', '2015_2019', 2, 8) successful!  ==>109/54000\n",
      "Unpack ('华东', '2015_2019', 2, 9) successful!  ==>110/54000\n",
      "Unpack ('华东', '2015_2019', 2, 10) successful!  ==>111/54000\n",
      "Unpack ('华东', '2015_2019', 2, 11) successful!  ==>112/54000\n",
      "Unpack ('华东', '2015_2019', 2, 12) successful!  ==>113/54000\n",
      "Unpack ('华东', '2015_2019', 2, 13) successful!  ==>114/54000\n",
      "Unpack ('华东', '2015_2019', 2, 14) successful!  ==>115/54000\n",
      "Unpack ('华东', '2015_2019', 2, 15) successful!  ==>116/54000\n",
      "Unpack ('华东', '2015_2019', 2, 16) successful!  ==>117/54000\n",
      "Unpack ('华东', '2015_2019', 2, 17) successful!  ==>118/54000\n",
      "Unpack ('华东', '2015_2019', 2, 18) successful!  ==>119/54000\n",
      "Unpack ('华东', '2015_2019', 2, 19) successful!  ==>120/54000\n",
      "Unpack ('华东', '2015_2019', 2, 20) successful!  ==>121/54000\n",
      "Unpack ('华东', '2015_2019', 2, 21) successful!  ==>122/54000\n",
      "Unpack ('华东', '2015_2019', 2, 22) successful!  ==>123/54000\n",
      "Unpack ('华东', '2015_2019', 2, 23) successful!  ==>124/54000\n",
      "Unpack ('华东', '2015_2019', 2, 24) successful!  ==>125/54000\n",
      "Unpack ('华东', '2015_2019', 2, 25) successful!  ==>126/54000\n",
      "Unpack ('华东', '2015_2019', 2, 26) successful!  ==>127/54000\n",
      "Unpack ('华东', '2015_2019', 2, 27) successful!  ==>128/54000\n",
      "Unpack ('华东', '2015_2019', 2, 28) successful!  ==>129/54000\n",
      "Unpack ('华东', '2015_2019', 2, 29) successful!  ==>130/54000\n",
      "Unpack ('华东', '2015_2019', 2, 30) successful!  ==>131/54000\n",
      "Unpack ('华东', '2015_2019', 2, 31) successful!  ==>132/54000\n",
      "Unpack ('华东', '2015_2019', 2, 32) successful!  ==>133/54000\n",
      "Unpack ('华东', '2015_2019', 2, 33) successful!  ==>134/54000\n",
      "Unpack ('华东', '2015_2019', 2, 34) successful!  ==>135/54000\n",
      "Unpack ('华东', '2015_2019', 2, 35) successful!  ==>136/54000\n",
      "Unpack ('华东', '2015_2019', 2, 36) successful!  ==>137/54000\n",
      "Unpack ('华东', '2015_2019', 2, 37) successful!  ==>138/54000\n",
      "Unpack ('华东', '2015_2019', 2, 38) successful!  ==>139/54000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unpack ('华东', '2015_2019', 2, 39) successful!  ==>140/54000\n",
      "Unpack ('华东', '2015_2019', 2, 40) successful!  ==>141/54000\n",
      "Unpack ('华东', '2015_2019', 2, 41) successful!  ==>142/54000\n",
      "Unpack ('华东', '2015_2019', 2, 42) successful!  ==>143/54000\n",
      "Unpack ('华东', '2015_2019', 2, 43) successful!  ==>144/54000\n",
      "Unpack ('华东', '2015_2019', 2, 44) successful!  ==>145/54000\n",
      "Unpack ('华东', '2015_2019', 2, 45) successful!  ==>146/54000\n",
      "Unpack ('华东', '2015_2019', 2, 46) successful!  ==>147/54000\n",
      "Unpack ('华东', '2015_2019', 2, 47) successful!  ==>148/54000\n",
      "Unpack ('华东', '2015_2019', 2, 48) successful!  ==>149/54000\n",
      "Unpack ('华东', '2015_2019', 2, 49) successful!  ==>150/54000\n",
      "Unpack ('华东', '2015_2019', 2, 50) successful!  ==>151/54000\n",
      "Unpack ('华东', '2015_2019', 2, 51) successful!  ==>152/54000\n",
      "Unpack ('华东', '2015_2019', 2, 52) successful!  ==>153/54000\n",
      "Unpack ('华东', '2015_2019', 2, 53) successful!  ==>154/54000\n",
      "Unpack ('华东', '2015_2019', 2, 54) successful!  ==>155/54000\n",
      "Unpack ('华东', '2015_2019', 2, 55) successful!  ==>156/54000\n",
      "Unpack ('华东', '2015_2019', 2, 56) successful!  ==>157/54000\n",
      "Unpack ('华东', '2015_2019', 2, 57) successful!  ==>158/54000\n",
      "Unpack ('华东', '2015_2019', 2, 58) successful!  ==>159/54000\n",
      "Unpack ('华东', '2015_2019', 2, 59) successful!  ==>160/54000\n",
      "Unpack ('华东', '2015_2019', 2, 60) successful!  ==>161/54000\n",
      "Unpack ('华东', '2015_2019', 2, 61) successful!  ==>162/54000\n",
      "Unpack ('华东', '2015_2019', 2, 62) successful!  ==>163/54000\n",
      "Unpack ('华东', '2015_2019', 2, 63) successful!  ==>164/54000\n",
      "Unpack ('华东', '2015_2019', 2, 64) successful!  ==>165/54000\n",
      "Unpack ('华东', '2015_2019', 2, 65) successful!  ==>166/54000\n",
      "Unpack ('华东', '2015_2019', 2, 66) successful!  ==>167/54000\n",
      "Unpack ('华东', '2015_2019', 2, 67) successful!  ==>168/54000\n",
      "Unpack ('华东', '2015_2019', 2, 68) successful!  ==>169/54000\n",
      "Unpack ('华东', '2015_2019', 2, 69) successful!  ==>170/54000\n",
      "Unpack ('华东', '2015_2019', 2, 70) successful!  ==>171/54000\n",
      "Unpack ('华东', '2015_2019', 2, 71) successful!  ==>172/54000\n",
      "Unpack ('华东', '2015_2019', 2, 72) successful!  ==>173/54000\n",
      "Unpack ('华东', '2015_2019', 2, 73) successful!  ==>174/54000\n",
      "Unpack ('华东', '2015_2019', 2, 74) successful!  ==>175/54000\n",
      "Unpack ('华东', '2015_2019', 2, 75) successful!  ==>176/54000\n",
      "Unpack ('华东', '2015_2019', 2, 76) successful!  ==>177/54000\n",
      "Unpack ('华东', '2015_2019', 2, 77) successful!  ==>178/54000\n",
      "Unpack ('华东', '2015_2019', 2, 78) successful!  ==>179/54000\n",
      "Unpack ('华东', '2015_2019', 2, 79) successful!  ==>180/54000\n",
      "Unpack ('华东', '2015_2019', 2, 80) successful!  ==>181/54000\n",
      "Unpack ('华东', '2015_2019', 2, 81) successful!  ==>182/54000\n",
      "Unpack ('华东', '2015_2019', 2, 82) successful!  ==>183/54000\n",
      "Unpack ('华东', '2015_2019', 2, 83) successful!  ==>184/54000\n",
      "Unpack ('华东', '2015_2019', 2, 84) successful!  ==>185/54000\n",
      "Unpack ('华东', '2015_2019', 2, 85) successful!  ==>186/54000\n",
      "Unpack ('华东', '2015_2019', 2, 86) successful!  ==>187/54000\n",
      "Unpack ('华东', '2015_2019', 2, 87) successful!  ==>188/54000\n",
      "Unpack ('华东', '2015_2019', 2, 88) successful!  ==>189/54000\n",
      "Unpack ('华东', '2015_2019', 2, 89) successful!  ==>190/54000\n",
      "Unpack ('华东', '2015_2019', 2, 90) successful!  ==>191/54000\n",
      "Unpack ('华东', '2015_2019', 2, 91) successful!  ==>192/54000\n",
      "Unpack ('华东', '2015_2019', 2, 92) successful!  ==>193/54000\n",
      "Unpack ('华东', '2015_2019', 2, 93) successful!  ==>194/54000\n",
      "Unpack ('华东', '2015_2019', 2, 94) successful!  ==>195/54000\n",
      "Unpack ('华东', '2015_2019', 2, 95) successful!  ==>196/54000\n",
      "Unpack ('华东', '2015_2019', 2, 96) successful!  ==>197/54000\n",
      "Unpack ('华东', '2015_2019', 2, 97) successful!  ==>198/54000\n",
      "Unpack ('华东', '2015_2019', 2, 98) successful!  ==>199/54000\n",
      "Unpack ('华东', '2015_2019', 2, 99) successful!  ==>200/54000\n",
      "Unpack ('华东', '2015_2019', 3, 0) successful!  ==>201/54000\n",
      "Unpack ('华东', '2015_2019', 3, 1) successful!  ==>202/54000\n",
      "Unpack ('华东', '2015_2019', 3, 2) successful!  ==>203/54000\n",
      "Unpack ('华东', '2015_2019', 3, 3) successful!  ==>204/54000\n"
     ]
    }
   ],
   "source": [
    "# set opration flag to report the process\n",
    "ops_num   = 0\n",
    "ops_total = len(regions) * len(year_range) * harmo_num * point_num\n",
    "\n",
    "# initiate an empty list to hold the residule_dfs\n",
    "Residule = []\n",
    "\n",
    "# loop through the dict and unpack the values\n",
    "for key in point_with_residule.keys():\n",
    "\n",
    "    # unpack the value\n",
    "    residule = point_with_residule[key].getInfo()\n",
    "\n",
    "    # construct the redidule df\n",
    "    col = residule[0]\n",
    "    val = residule[1]\n",
    "    df = pd.DataFrame([val],columns = col,index=[key])\n",
    "\n",
    "    # add the df to list\n",
    "    Residule.append(df)\n",
    "\n",
    "    # print out the process\n",
    "    ops_num = ops_num + 1\n",
    "    print(f'Unpack {key} successful!  ==>{ops_num}/{ops_total}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step_3_Read local csv and make plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Save the result to local drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save, comment this line in case overwrriting\n",
    "#Residule_df = pd.DataFrame.from_dict(Residule).T\n",
    "#Residule_df.to_csv('Step_2_result/Residule.csv')\n",
    "\n",
    "# read from csv\n",
    "Residule_df = pd.read_csv('./Reuslt/Residule.csv')\n",
    "Residule_df.columns = ['Year', 'Harmonic', 'Point', 'Index', 'Value']\n",
    "\n",
    "\n",
    "# add a time span to df\n",
    "Residule_df['Span'] = Residule_df['Year'].apply(lambda x: int(x.split('_')[1]) - int(x.split('_')[0]) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Count the Nan value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the rows where Value == Nan\n",
    "Nan_rows = Residule_df[Residule_df['Value'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute how many nan values exist in differnet year_range\n",
    "Count_nan_year = collections.Counter(Nan_rows['Year'])\n",
    "\n",
    "# compute how many nan values exist in differnet harmonic\n",
    "Count_nan_hamonic = collections.Counter(Nan_rows['Harmonic'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(Count_nan_year)\n",
    "print(Count_nan_hamonic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Reshape the dataframe and make figures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add [Mean-Err] and romove [Value,Point]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a column indicating which sensor is\n",
    "Landsat_8_range = ['2015_2019', '2016_2019', '2017_2019', '2018_2019', '2019_2019']\n",
    "Landsat_7_range = ['2009_2013', '2010_2013', '2011_2013', '2012_2013', '2013_2013']\n",
    "Landsat_5_range = ['2006_2010', '2007_2010', '2008_2010', '2009_2010', '2010_2010']\n",
    "\n",
    "# perform the test using nest if else statement.\n",
    "Residule_df['Sensor'] = Residule_df['Year'].apply(lambda x: 'Landsat_8' if x in Landsat_8_range \n",
    "                                                                           else ('Landsat_7' if x in Landsat_7_range \n",
    "                                                                             else 'Landsat_5'))\n",
    "\n",
    "Residule_df['Mean_Err'] = Residule_df['Value']/Residule_df['Span']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reshape the Residule_df into <br>\n",
    "1) grouped --> Mean of [Value]  with indicate the magitude of residule <br>\n",
    "2) single_index --> the single-indexed version of groupd_df, which is used for sns figure making  <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group the dafaframe with point_id and a function of mean\n",
    "grouped = Residule_df.drop(['Point','Value'],axis=1).groupby(['Sensor','Span','Harmonic','Index']).mean()\n",
    "single_index = grouped.reset_index(level=[0,1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Observe that Hamonic of 3 (3 pairs of sinuate functions) is good enough to reduce mean error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x='Harmonic',y = 'Mean_Err',data = single_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Observe that 3 years composite reduces the mean error significantly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(data = single_index[single_index['Harmonic'] == 3],\n",
    "            x='Span',\n",
    "            y = 'Mean_Err')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
